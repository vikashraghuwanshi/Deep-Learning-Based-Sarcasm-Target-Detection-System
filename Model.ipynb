{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense ,LSTM,concatenate,Input,Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv('dataset.csv')\n",
    "colnames=['Snippets', 'Targets'] \n",
    "df = pd.read_csv('dataset.csv', names=colnames, header=None)\n",
    "ppd=pd.read_csv('pre_processed_dataset.csv',encoding='cp1252')\n",
    "\n",
    "f = open(b\"Data_fast.pkl\",\"rb\")\n",
    "keras_left_context, keras_right_context, keras_middle, labels = zip(*pickle.load(f))\n",
    "f.close()\n",
    "\n",
    "f = open(b\"Data_aug.pkl\",\"rb\")\n",
    "pos_vec, ner_vec, empath_vec = zip(*pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.shape(keras_left_context))\n",
    "#print(np.shape(keras_right_context))\n",
    "#print(np.shape(keras_middle))\n",
    "#print(np.shape(pos_vec))\n",
    "#print(np.shape(ner_vec))\n",
    "#print(np.shape(liwc_vec))\n",
    "#print(np.shape(empath_vec))\n",
    "\n",
    "#Tuned-Hyper Parameters\n",
    "embed_size = 1024\n",
    "hidden_size = 32\n",
    "num_epochs=30\n",
    "layer_size = 16\n",
    "batch_size = 64\n",
    "mode = 'Uni' # 'Uni' : Unidirectional LSTM  |  'Bi' : Bidirectional LSTM  | 'TD' : Target-dependent LSTM\n",
    "augmentation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compress candidate-words belonging to the same tweet/line togerather\n",
    "def compress():\n",
    "    lengths = []\n",
    "    for i in range (1,len(ppd)):\n",
    "        if ppd['left_context'][i][2:-2] == \"<start>\":\n",
    "            lengths.append(i)\n",
    "    lengths.append(len(ppd))\n",
    "    compressor = []\n",
    "    compressor.append(range(lengths[0]))\n",
    "    for i in range (1,len(lengths)):\n",
    "        compressor.append(range(lengths[i-1],lengths[i]))\n",
    "    return compressor\n",
    "comp = compress()\n",
    "\n",
    "# Dataset division for 3-fold cross validation\n",
    "indices = list (range (len(comp)))\n",
    "np.random.shuffle(indices)\n",
    "bins = []\n",
    "bins.append(indices[:int(0.33*len(indices))])\n",
    "bins.append(indices[int(0.33*len(indices)):int(0.66*len(indices))])\n",
    "bins.append(indices[int(0.66*len(indices)):])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data, Test Data Preparation\n",
    "\n",
    "def prep (train_indices, test_indices):\n",
    "    print (len(train_indices), len(test_indices))\n",
    "\n",
    "    train_ids = []\n",
    "    for i in range(len(train_indices)):\n",
    "        train_ids.extend(comp[train_indices[i]])\n",
    "\n",
    "    test_ids = []\n",
    "    for i in range(len(test_indices)):\n",
    "        test_ids.extend(comp[test_indices[i]])\n",
    "\n",
    "    # Training Data Preparion\n",
    "    train_left   = []\n",
    "    train_right  = []\n",
    "    train_middle = []\n",
    "    train_labels = []\n",
    "    train_pos_vec= []\n",
    "    train_neg_vec= []\n",
    "    #train_liwc_vec=[]\n",
    "    train_empath_vec=[]\n",
    "\n",
    "    for id in train_ids:\n",
    "        train_left.append(keras_left_context[id])\n",
    "        train_right.append(keras_right_context[id])\n",
    "        train_middle.append(keras_middle[id])\n",
    "        train_labels.append(labels[id])\n",
    "        train_pos_vec.append(pos_vec[id])\n",
    "        train_neg_vec.append(ner_vec[id])\n",
    "        #train_liwc_vec.append(liwc_vec[id])\n",
    "        train_empath_vec.append(empath_vec[id])\n",
    "\n",
    "    train_left   = np.array(train_left)\n",
    "    train_right  = np.array(train_right)\n",
    "    train_middle = np.array(train_middle)\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_middle = np.expand_dims(train_middle,axis=1)\n",
    "    train_pos_vec= np.array(train_pos_vec)\n",
    "    train_neg_vec= np.array(train_neg_vec)\n",
    "    #train_liwc_vec= np.array(train_liwc_vec)\n",
    "    train_empath_vec= np.array(train_empath_vec)\n",
    "\n",
    "    # Training Data Preparion\n",
    "    val_left   = []\n",
    "    val_right  = []\n",
    "    val_middle = []\n",
    "    val_labels = []\n",
    "    val_pos_vec= []\n",
    "    val_ner_vec= []\n",
    "    #val_liwc_vec=[]\n",
    "    val_empath_vec=[]\n",
    "\n",
    "    for id in test_ids:\n",
    "        val_left.append(keras_left_context[id])\n",
    "        val_right.append(keras_right_context[id])\n",
    "        val_middle.append(keras_middle[id])\n",
    "        val_labels.append(labels[id])\n",
    "        val_pos_vec.append(pos_vec[id])\n",
    "        val_ner_vec.append(ner_vec[id])\n",
    "        #val_liwc_vec.append(liwc_vec[id])\n",
    "        val_empath_vec.append(empath_vec[id])\n",
    "\n",
    "    val_left   = np.array(val_left)\n",
    "    val_right  = np.array(val_right)\n",
    "    val_middle = np.array(val_middle)\n",
    "    val_labels = np.array(val_labels)\n",
    "    val_middle = np.expand_dims(val_middle, axis=1)\n",
    "    val_pos_vec=np.array(val_pos_vec)\n",
    "    val_ner_vec=np.array(val_ner_vec)\n",
    "    #val_liwc_vec=np.array(val_liwc_vec)\n",
    "    val_empath_vec=np.array(val_empath_vec)\n",
    "\n",
    "    # Below part only for TD lstm\n",
    "    if mode == 'TD':\n",
    "        train_left = np.concatenate((train_left, train_middle), axis=1)\n",
    "        train_right = np.concatenate((train_middle, train_right), axis=1)\n",
    "        val_left = np.concatenate((val_left, val_middle), axis=1)\n",
    "        val_right = np.concatenate((val_middle, val_right), axis=1)\n",
    "    return(train_left,train_right,train_middle,train_pos_vec,train_neg_vec,train_empath_vec,train_labels,val_left,val_right,val_middle,val_pos_vec,val_ner_vec,val_empath_vec,val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_comp(arr, test_indices):\n",
    "    arr = deque(arr)\n",
    "    fin = []\n",
    "    for i in test_indices:\n",
    "        temp = []\n",
    "        for j in range(len(comp[i])):\n",
    "            temp.append(arr.popleft())\n",
    "        fin.append(temp)\n",
    "    return(fin) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy (pred, labels, test_indices):\n",
    "    pred = pred[0]\n",
    "    num_sent = len(comp)\n",
    "    num_words = len(pred)\n",
    "    threshold = 0\n",
    "    cnt = 0\n",
    "    # Threshold calculation for binary classification problem\n",
    "    for a,b in zip (pred,labels):\n",
    "        if b==1.0:\n",
    "            threshold+=a\n",
    "            cnt+=1\n",
    "    threshold = threshold.item()/cnt\n",
    "\n",
    "    pred_th = []\n",
    "    for x in pred:\n",
    "        if (x<=threshold):\n",
    "            pred_th.append(0)\n",
    "        else :\n",
    "            pred_th.append(1)\n",
    "\n",
    "    pred_th = np.array(pred_th)\n",
    "    print (\"Number of Test sentences : {}\".format(len(test_indices)))\n",
    "    error = pred_th-labels\n",
    "    error_d  = de_comp(error,test_indices)\n",
    "    labels_d = de_comp(labels,test_indices)\n",
    "    pred_d   = de_comp(pred_th,test_indices)\n",
    "    em_cnt = 0\n",
    "    ds_cnt = 0\n",
    "    mic_f1 = 0\n",
    "\n",
    "    for err in error_d:\n",
    "        if (sum(err)==0):\n",
    "           em_cnt += 1\n",
    "        ds_cnt += float(len(err)-sum(np.abs(err)))/len(err)\n",
    "\n",
    "    for lab, pre in zip(labels_d,pred_d):\n",
    "\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        tn = 0\n",
    "\n",
    "        for i,j in zip(lab,pre):\n",
    "            if (int(i) == 1)  and (int(j) ==0) :\n",
    "                fn += 1\n",
    "            elif (int(i) == 0)  and (int(j) ==1) :\n",
    "                fp += 1\n",
    "            elif (int(i) == 0)  and (int(j) ==0) :\n",
    "                tn += 1\n",
    "            elif (int(i) == 1) and (int(j) ==1):\n",
    "                tp += 1\n",
    "        try : \n",
    "            mic_f1 += float(2*tp) / (2*tp + fn +fp)\n",
    "        except :\n",
    "            pass\n",
    "\n",
    "    TP=0\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    for a,b in zip(labels, pred_th):\n",
    "\n",
    "        if int(a)==0 and b==0:\n",
    "            TN+=1\n",
    "        if int(a)==1 and b==1:\n",
    "            TP+=1\n",
    "        if int(a)==0 and b==1:\n",
    "            FP+=1\n",
    "        if int(a)==1 and b==0:\n",
    "            FN+=1 \n",
    "    print (\"TP = {}, TN = {},FP = {}, FN = {}\".format(TP,TN,FP,FN))\n",
    "    F1 = float(2*TP)/(2*TP + FP+ FN)\n",
    "    EM = float(em_cnt)/len(test_indices)\n",
    "    DS = float(ds_cnt)/len(test_indices)\n",
    "    uF1= float(mic_f1)/len(test_indices)\n",
    "    print (\"EM Accuracy : {}\".format(EM))\n",
    "    print (\"DS Accuracy : {}\".format(DS))\n",
    "    print (\"Micro F1    : {}\".format(uF1))\n",
    "    print (\"Macro F1 Score = {}\".format(F1))\n",
    "    return (pred_d, labels_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_l,train_r,train_m,train_pos,train_ner,train_empath,train_labels,test_l,test_r,test_m,test_pos,test_ner,test_empath,test_labels,test_indices):\n",
    "\n",
    "    x=Input(shape=(None,embed_size))\n",
    "    y=Input(shape=(None,embed_size))\n",
    "    z=Input(shape=(None,embed_size))\n",
    "    z1=Input(shape=([34]))\n",
    "    z2=Input(shape=([4]))\n",
    "    #z3=Input(shape=([64]))\n",
    "    z4=Input(shape=([194]))\n",
    "\n",
    "    if mode == 'Bi':\n",
    "        left_out=Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_l.shape[1:]))(x)      \n",
    "        middle = Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_m.shape[1:]))(y)\n",
    "        right_out=Bidirectional(LSTM(hidden_size//2,return_sequences=False),input_shape=(train_r.shape[1:]))(z)\n",
    "\n",
    "    else:\n",
    "        left_out  = LSTM(hidden_size,return_sequences=False)(x)\n",
    "        middle    = LSTM(hidden_size,return_sequences=False)(y)\n",
    "        right_out = LSTM(hidden_size,return_sequences=False)(y)\n",
    "\n",
    "    pos_dense=Dense(32,activation='relu')(z1)\n",
    "    ner_dense=Dense(16,activation='relu')(z2)\n",
    "    #liwc_dense=Dense(64,activation='relu')(z3)\n",
    "    empath_dense=Dense(64,activation='relu')(z4)\n",
    "\n",
    "    if mode == 'TD' and augmentation == False :\n",
    "        out=concatenate([left_out,right_out],axis=-1)\n",
    "\n",
    "    if mode == 'TD' and augmentation == True :\n",
    "        out=concatenate([left_out,right_out,pos_dense,ner_dense,empath_dense],axis=-1)\n",
    "\n",
    "    if mode != 'TD' and augmentation == False :\n",
    "        out=concatenate([left_out,middle,right_out],axis=-1)\n",
    "\n",
    "    if mode != 'TD' and augmentation == True :\n",
    "        out=concatenate([left_out,middle,right_out,pos_dense,ner_dense,empath_dense],axis=-1)\n",
    "\n",
    "    out=Dense(layer_size, activation='relu')(out)\n",
    "    output=Dense(1, activation='sigmoid')(out)\n",
    "    model = Model(inputs=[x,y,z,z1,z2,z4], outputs=output)\n",
    "    model.compile(optimizer=Adam(lr=10e-5),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    print (\"Starting Epochs\")\n",
    "    for i in range(num_epochs):\n",
    "        model.fit([train_l,train_r,train_m,train_pos,train_ner,train_empath],train_labels,batch_size=batch_size, epochs=1,verbose=0)\n",
    "        print('***************************************************************')\n",
    "        print (\"predicting_ Epoch : {}\".format(i))\n",
    "        pred_val=[]\n",
    "        pred_val.append(model.predict([test_l,test_r,test_m,test_pos,test_ner,test_empath]))\n",
    "        pre_d, lab_d = accuracy (pred_val, test_labels,test_indices)\n",
    "\n",
    "        with open('Tweets Aug-{}.csv'.format(i), mode='w') as file:\n",
    "            file_writer = csv.writer(file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            for a,b in zip (pre_d,lab_d):\n",
    "                file_writer.writerow([a,b])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "0 1\n",
      "0 1\n",
      "Saved model to disk\n",
      "Fold 2\n",
      "0 1\n",
      "1 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-1bddc0295712>:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val_left   = np.array(val_left)\n",
      "<ipython-input-14-1bddc0295712>:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  val_right  = np.array(val_right)\n",
      "<ipython-input-14-1bddc0295712>:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  train_left   = np.array(train_left)\n",
      "<ipython-input-14-1bddc0295712>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  train_right  = np.array(train_right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Fold 3\n",
      "0 1\n",
      "1 0\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "for i in range (3):\n",
    "    print (\"Fold {}\".format(i+1))\n",
    "    print (len(bins[0] + bins[1]),len(bins[2]))\n",
    "    train_left,train_right,train_middle,pos_vec_train,ner_vec_train,empath_vec_train,train_labels,val_left,val_right,val_middle,pos_vec_val,ner_vec_val,empath_vec_val,val_labels = prep (bins[i%3] + bins[(i+1)%3], bins[(i+2)%3])\n",
    "    #Sar_model=model(train_left,train_right,train_middle,pos_vec_train,ner_vec_train,empath_vec_train,train_labels,val_left,val_right,val_middle,pos_vec_val,ner_vec_val,empath_vec_val,val_labels,bins[(i+2)%3])\n",
    "    #Sar_model.save_weights(\"Bert Tweets Aug.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
